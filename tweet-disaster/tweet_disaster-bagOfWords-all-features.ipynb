{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af6d9caf-5605-4426-a0d1-0bdd79f50dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faf83a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enhanced Text Preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove @ mentions and hashtag symbol\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # Remove punctuation\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481725ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced categorical feature preprocessing\n",
    "encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_ds['keyword'] = train_ds['keyword'].fillna('unknown')\n",
    "train_ds['location'] = train_ds['location'].fillna('unknown')\n",
    "\n",
    "train_ds['keyword'] = encoder.fit_transform(train_ds['keyword'])\n",
    "train_ds['location'] = encoder.fit_transform(train_ds['location'])\n",
    "\n",
    "train_ds['keyword'] = scaler.fit_transform(train_ds[['keyword']])\n",
    "train_ds['location'] = scaler.fit_transform(train_ds[['location']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c96ded-8823-4cd5-8655-0aa648741c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "train_ds.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddfab62d-030b-42ec-841e-afd12a19a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Split with Stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_ds.drop('target', axis=1, inplace=False)\n",
    "y = train_ds['target'].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2710cda-4400-4449-86d9-8ed2ca796c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Text Vectorization\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1f87dee-7167-47c1-9d0c-b91dadf52f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with enhanced preprocessing\n",
    "def prepare_features(X_data, y_data):\n",
    "    text_data = text_vectorization(X_data['text'])\n",
    "    cat_data = X_data[['keyword', 'location']].values.astype('float32')\n",
    "    return text_data, cat_data, y_data\n",
    "\n",
    "X_train_text, X_train_cat, y_train = prepare_features(X_train, y_train)\n",
    "X_val_text, X_val_cat, y_val = prepare_features(X_val, y_val)\n",
    "X_test_text, X_test_cat, y_test = prepare_features(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf7ecbc6-1d20-4bbe-9d3b-735cbe734250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Architecture\n",
    "def get_enhanced_model(max_tokens=20000, embed_dim=128, num_heads=2):\n",
    "    # Text input branch\n",
    "    text_input = layers.Input(shape=(50,), name='text_input')\n",
    "    x = layers.Embedding(max_tokens, embed_dim)(text_input)\n",
    "    \n",
    "    # Add transformer encoder block\n",
    "    transformer_block = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    x = transformer_block(x, x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Categorical features branch\n",
    "    cat_input = layers.Input(shape=(2,), name='cat_input')\n",
    "    cat_features = layers.Dense(32, activation='relu')(cat_input)\n",
    "    \n",
    "    # Combine features\n",
    "    combined = layers.concatenate([x, cat_features])\n",
    "    \n",
    "    # Dense layers with residual connections\n",
    "    dense1 = layers.Dense(64, activation='relu')(combined)\n",
    "    dense1 = layers.Dropout(0.3)(dense1)\n",
    "    dense2 = layers.Dense(32, activation='relu')(dense1)\n",
    "    dense2 = layers.Dropout(0.2)(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=[text_input, cat_input], outputs=output)\n",
    "    \n",
    "    # Compile with enhanced optimizer and learning rate schedule    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'AUC']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aee37030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train enhanced model\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train_text, X_train_cat), y_train))\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((X_val_text, X_val_cat), y_val))\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((X_test_text, X_test_cat), y_test))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create model\n",
    "model = get_enhanced_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a833b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add enhanced callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"enhanced_disaster_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e6fdf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - AUC: 0.5306 - accuracy: 0.5494 - loss: 0.6803 - val_AUC: 0.8292 - val_accuracy: 0.7255 - val_loss: 0.5458 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.8673 - accuracy: 0.8074 - loss: 0.4321 - val_AUC: 0.8576 - val_accuracy: 0.7948 - val_loss: 0.4769 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9507 - accuracy: 0.8942 - loss: 0.2759 - val_AUC: 0.8491 - val_accuracy: 0.7797 - val_loss: 0.6317 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9788 - accuracy: 0.9445 - loss: 0.1696 - val_AUC: 0.8369 - val_accuracy: 0.7654 - val_loss: 0.7932 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9905 - accuracy: 0.9645 - loss: 0.1067 - val_AUC: 0.8100 - val_accuracy: 0.7235 - val_loss: 1.4269 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e97fe0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.8837 - accuracy: 0.8241 - loss: 0.4141\n",
      "\n",
      "Test accuracy: 0.816\n",
      "Test AUC: 0.872\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "model = keras.models.load_model(\"enhanced_disaster_model.keras\")\n",
    "test_results = model.evaluate(test_dataset)\n",
    "print(f\"\\nTest accuracy: {test_results[1]:.3f}\")\n",
    "print(f\"Test AUC: {test_results[2]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
