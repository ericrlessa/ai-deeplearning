{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10519733,"sourceType":"datasetVersion","datasetId":6511228}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<span style=\" color: yellow; font-size: 24px;\">Deep Learning 1 Project _ Group 5:<br>Tweet Sentiment Extraction (Kaggle Â· Featured Code Competition)<br></span><span style=\"font-size: 22px;\">\nShrey\tPatel\t101541370<br>\nSam\tEmami\t101575471<br>\nEric\tLessa\t101549935<br>\nDwip\tMakwana\t101483523<br>\nMoossa\tHussain\t101542820<br>\nChaoyu\tLiu\t101573622<br>\nDevanshi \tDave\t101582208<br>\nRutika\tBhuva\t101551781<br>\n</span>","metadata":{"id":"ZHAenzcx-t9U"}},{"cell_type":"markdown","source":"<span style=\" color: yellow; font-size: 24px;\">Use This Template **by memeber name**: </span>description for each part<span style=\"font-size: 22px;\">","metadata":{"id":"8woVWkoG-t9W"}},{"cell_type":"code","source":"!pip install shap\n!pip install transformers\n!pip install pipeline\n!pip install datasets\n!pip show torchvision\n!pip install evaluate\n\n#go to https://graphviz.gitlab.io/download/ and install graphviz","metadata":{"execution":{"iopub.status.busy":"2025-01-29T12:29:01.348755Z","iopub.execute_input":"2025-01-29T12:29:01.349071Z","iopub.status.idle":"2025-01-29T12:29:21.567788Z","shell.execute_reply.started":"2025-01-29T12:29:01.349030Z","shell.execute_reply":"2025-01-29T12:29:21.566794Z"},"id":"SCAGkvve-t9W","trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.2)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->shap) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->shap) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->shap) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->shap) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->shap) (2024.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCollecting pipeline\n  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\nDownloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\nInstalling collected packages: pipeline\nSuccessfully installed pipeline-0.1.0\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nName: torchvision\nVersion: 0.20.1+cu121\nSummary: image and video datasets and models for torch deep learning\nHome-page: https://github.com/pytorch/vision\nAuthor: PyTorch Core Team\nAuthor-email: soumith@pytorch.org\nLicense: BSD\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: numpy, pillow, torch\nRequired-by: easyocr, fastai, timm\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<span style=\"color: yellow; font-size: 24px;\">Hugging Face Transformers Tools for Using Pre-trained LLMs:<br><br></span>\n<span style=\"color: yellow; font-size: 24px;\">pipeline: </span><span style=\"font-size: 22px;\"> It is like a convenient wrapper or helper that simplifies the process of working with models such as those loaded with AutoModelForSequenceClassification and their associated tokenizers.<br>It integrates with pre-trained models and tokenizers for specific tasks and perform NLP tasks and the processes such as:<br>\nLoading the model.<br>\nTokenizing the input text.<br>\nGenerating predictions.<br>\nPost-processing the output (if necessary)<br><br></span>\n<span style=\"color: yellow; font-size: 24px;\">AutoModelForSequenceClassification: </span><span style=\"font-size: 22px;\"> Loading Pre-trained Models designed for sequence classification tasks. Sequence Classification refers to tasks where you classify an entire sequence of text (like a sentence or document) into a category. Examples include:<br>\n<span style=\"color: red\">Sentiment analysis (positive, negative, neutral)</span><br>\nTopic classification (news, sports, politics)<br>\nSpam detection<br>\nIntent classification (e.g., order, question, complaint)<br>\n\"Auto\" in the Name: The Auto prefix in the class name signifies its flexibility. You can use it to load various pre-trained models from different architectures (BERT, RoBERTa, DistilBERT, etc.) simply by specifying the model name or path.<br><br></span>\n<span style=\"color: yellow; font-size: 24px;\">AutoTokenizer: </span><span style=\"font-size: 22px;\"> tokenizing the inputs. The tokenizer ensures that:<br>\nThe tokenized input matches the vocabulary and format expected by the model.<br>\nThe inputs include features like input_ids, attention_mask, and token_type_ids (if applicable).<br><br></span>","metadata":{"id":"vXZGn0sW-t9Y"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport shap\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nimport evaluate\nfrom transformers import EarlyStoppingCallback\nfrom transformers import EvalPrediction\nfrom transformers import (\n    pipeline,\n    AutoModelForSequenceClassification,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n    DefaultDataCollator,\n    DataCollatorWithPadding,\n    AutoTokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2025-01-29T12:29:31.185663Z","iopub.execute_input":"2025-01-29T12:29:31.185961Z","iopub.status.idle":"2025-01-29T12:29:54.696947Z","shell.execute_reply.started":"2025-01-29T12:29:31.185936Z","shell.execute_reply":"2025-01-29T12:29:54.696025Z"},"id":"y-CwZu9W-t9Z","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Disable wandb\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T12:29:54.698157Z","iopub.execute_input":"2025-01-29T12:29:54.698786Z","iopub.status.idle":"2025-01-29T12:29:54.702251Z","shell.execute_reply.started":"2025-01-29T12:29:54.698760Z","shell.execute_reply":"2025-01-29T12:29:54.701451Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SentimentExtraction:\n    \n    def __init__(self, train_path):\n        self.train_df = pd.read_csv(train_path)\n        self.tokenizer = None\n        self.model = None\n        self.squad_metric = evaluate.load(\"squad\")\n\n    def prepare_data(self):\n        \"\"\"Clean and prepare the data\"\"\"\n        print(\"Dataset columns:\", self.train_df.columns.tolist())\n        print(\"\\nSample data:\")\n        print(self.train_df.head())\n        \n        self.train_df = self.train_df.dropna(subset=['text'])\n        \n        dataset = Dataset.from_pandas(self.train_df)\n        split_data = dataset.train_test_split(test_size=0.1, seed=42)\n        return split_data['train'], split_data['test']\n\n    def setup_model(self, model_name):\n        \"\"\"Initialize the model and tokenizer\"\"\"\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # Initialize for binary classification (2 labels)\n        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        \n        # Move model to available device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model.to(device)\n\n    def prepare_train_features(self, examples):\n        sentiment = examples['sentiment']\n        tweet = examples['text']\n        span = examples['selected_text']\n\n        tokenized_qa = self.tokenizer(sentiment, # question\n                                tweet, # context\n                                padding='max_length',\n                                return_offsets_mapping=True)\n\n        qa_tokens = tokenized_qa[\"input_ids\"]\n\n        span_tokens = self.tokenizer(span)[\"input_ids\"]\n\n        tokenized_qa[\"start_positions\"] = []\n        tokenized_qa[\"end_positions\"] = []\n\n        start_char = tweet.find(span)\n        end_char = start_char + len(span)\n\n        offsets = tokenized_qa.pop(\"offset_mapping\")\n        start_token = end_token = None\n        for idx, (start, end) in enumerate(offsets):\n            if start <= start_char < end:\n                start_token = idx\n            if start < end_char <= end:\n                end_token = idx\n                break\n\n        tokenized_qa[\"start_positions\"].append(start_token)\n        tokenized_qa[\"end_positions\"].append(end_token)\n\n        if start is None or end is None:\n            print(tweet + ' -> ' + span)\n            print(f' {qa_tokens} -> {span_tokens}')\n            return None\n\n        return tokenized_qa\n\n    def compute_metrics(self, eval_pred):\n        predictions, labels = eval_pred\n        start_preds, end_preds = predictions  # Unpack start and end logits\n        start_labels, end_labels = labels     # Unpack start and end positions\n\n        # Convert logits to predicted indices\n        start_preds = np.argmax(start_preds, axis=1)  # Shape: (100,)\n        end_preds = np.argmax(end_preds, axis=1)      # Shape: (100,)\n\n        total_jaccard = 0\n        num_samples = len(start_preds)\n\n        # Create prediction and reference text spans\n        preds = []\n        refs = []\n        for i in range(len(start_preds)):\n            # Decode predicted text span\n            pred_tokens = self.tokenizer.decode(range(start_preds[i], end_preds[i] + 1))            \n            preds.append({\"id\": str(i), \"prediction_text\": pred_tokens})\n            \n            # Decode reference text span\n            ref_tokens = self.tokenizer.decode(range(start_labels[i, 0], end_labels[i, 0] + 1))\n            refs.append({\"id\": str(i), \"answers\": {\"text\": [ref_tokens], \"answer_start\": [start_labels[i, 0]]}})\n\n            pred_set = set(pred_tokens.lower().split())\n            ref_set = set(ref_tokens.lower().split())\n            intersection = pred_set.intersection(ref_set)\n            union = pred_set.union(ref_set)\n            jaccard = len(intersection) / len(union) if union else 0\n            total_jaccard += jaccard\n\n        avg_jaccard = total_jaccard / num_samples\n\n        # Compute SQuAD metrics\n        squad_result = self.squad_metric.compute(predictions=preds, references=refs)\n\n        return {\n            \"exact_match\": f\"{squad_result['exact_match']:.2f}%\",\n            \"f1_score\": f\"{squad_result['f1']:.2f}%\",\n            \"avg_jaccard\": f\"{avg_jaccard:.2f}\"\n        }\n\n    def train(self, model_name, checkpoint=None):\n        # Prepare data\n        print(\"\\nPreparing datasets...\")\n        train_dataset, eval_dataset = self.prepare_data()\n        print(f\"Train dataset size: {len(train_dataset)}\")\n        print(f\"Eval dataset size: {len(eval_dataset)}\")\n\n        # Setup model\n        print(\"\\nSetting up model...\")\n        self.setup_model(model_name)\n\n        tokenized_train = train_dataset.map(self.prepare_train_features)\n\n        tokenized_eval = eval_dataset.map(self.prepare_train_features)\n\n        print(f\"\\nSelecting {tokenized_train} training samples and {tokenized_eval} evaluation samples...\")\n\n        args = TrainingArguments(\n            f\"finetune-BERT-tweet\",\n            evaluation_strategy = \"epoch\",\n            learning_rate=2e-5,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=3,\n            weight_decay=0.01,\n            save_total_limit=2,\n            fp16=True\n        )\n\n        #early_stopping = EarlyStoppingCallback(early_stopping_patience=3)  # Stop if no improvement after 3 evaluations\n\n        #small_train_dataset = tokenized_train.shuffle(seed=42).select(range(500))\n        #small_eval_dataset = tokenized_eval.shuffle(seed=42).select(range(100))\n\n        trainer = Trainer(\n            model=self.model,\n            args=args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_eval,\n            #train_dataset=small_train_dataset,\n            #eval_dataset=small_eval_dataset,\n            data_collator=DataCollatorWithPadding(self.tokenizer),\n            tokenizer=self.tokenizer,\n            compute_metrics=self.compute_metrics\n         #    callbacks=[early_stopping]\n        )\n\n        # Train model\n        print(\"\\nStarting training...\")\n        try:\n            trainer.train(resume_from_checkpoint=checkpoint)\n            print(\"\\nTraining completed successfully!\")\n        except Exception as e:\n            print(f\"\\nError during training: {str(e)}\")\n            return\n        \n        # Save the model\n        print(\"\\nSaving model...\")\n        trainer.save_model(\"sentiment_extraction_tweet_model\")\n\n    def baseline(self):\n\n        dataset, _ = self.prepare_data()\n\n        total_jaccard = 0\n        preds = []\n        refs = []\n        for i in range(len(dataset)):\n            sample = dataset[i]\n            sentiment = sample['sentiment']\n            tweet = sample['text']\n            span = sample['selected_text']\n    \n            inputs = self.tokenizer(\n                sentiment,  # the sentiment question\n                tweet,      # the tweet context\n                padding=True,\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n        \n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n            inputs = {k: v.to(device) for k, v in inputs.items()}\n           \n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n        \n                start_position = torch.argmax(start_logits, dim=-1)\n                end_position = torch.argmax(end_logits, dim=-1)\n        \n                tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n                \n                pred_span_tokens = tokens[start_position:end_position+1]\n                pred_text = self.tokenizer.convert_tokens_to_string(pred_span_tokens)\n\n                preds.append({\"id\": str(i), \"prediction_text\": pred_text})\n                refs.append({\"id\": str(i), \"answers\": {\"text\": [span], \"answer_start\": [tweet.find(span)]}})\n\n                set1 = set(pred_text.lower().split())\n                set2 = set(span.lower().split())\n                jaccard = len(set1 & set2) / len(set1 | set2)  # Intersection / Union\n                total_jaccard += jaccard\n        \n        squad_result = self.squad_metric.compute(predictions=preds, references=refs)\n        print(\"Prediction results without fine-tuning:\")\n        print(f\"\\texact_match: {squad_result['exact_match']:.2f}%\")\n        print(f\"\\tf1_score: {squad_result['f1']:.2f}%\")\n        print(f\"\\tavg_jaccard: {total_jaccard/len(dataset):.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:51:16.300122Z","iopub.execute_input":"2025-01-29T13:51:16.300487Z","iopub.status.idle":"2025-01-29T13:51:16.319925Z","shell.execute_reply.started":"2025-01-29T13:51:16.300453Z","shell.execute_reply":"2025-01-29T13:51:16.319054Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def main():\n    # Initialize processor\n    print(\"Initializing data processor...\")\n    processor = SentimentExtraction('/kaggle/input/tweet-extraction/train.csv')\n\n    #model_name = \"bert-base-uncased\"\n    model_name = \"distilbert-base-uncased\"\n    processor.train(model_name)\n\n    return processor\n\ndef prediction_test(processor):\n    tweet = \" you guys didn`t say hi or answer my questions yesterday  but nice songs.\"\n    sentiment = \"positive\"\n\n    inputs = processor.tokenizer(\n        sentiment,  # the sentiment question\n        tweet,      # the tweet context\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = processor.model(**inputs)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n        start_position = torch.argmax(start_logits, dim=-1)\n        end_position = torch.argmax(end_logits, dim=-1)\n\n        tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n        \n        sentiment_span = tokens[start_position:end_position+1]\n        sentiment_text = processor.tokenizer.convert_tokens_to_string(sentiment_span)\n        \n        print(f\"Predicted sentiment span: {sentiment_text}\")\n\ndef print_baseline():\n    # Initialize processor\n    print(\"Initializing data processor...\")\n    processor = SentimentExtraction('/kaggle/input/tweet-extraction/train.csv')\n\n    #model_name = \"bert-base-uncased\"\n    model_name = \"distilbert-base-uncased\"\n    processor.setup_model(model_name)\n    processor.baseline()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:31:30.343813Z","iopub.execute_input":"2025-01-29T13:31:30.344128Z","iopub.status.idle":"2025-01-29T13:31:30.350846Z","shell.execute_reply.started":"2025-01-29T13:31:30.344103Z","shell.execute_reply":"2025-01-29T13:31:30.349918Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"print_baseline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:51:20.980276Z","iopub.execute_input":"2025-01-29T13:51:20.980568Z","iopub.status.idle":"2025-01-29T13:53:16.734962Z","shell.execute_reply.started":"2025-01-29T13:51:20.980547Z","shell.execute_reply":"2025-01-29T13:53:16.733869Z"}},"outputs":[{"name":"stdout","text":"Initializing data processor...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Dataset columns: ['textID', 'text', 'selected_text', 'sentiment']\n\nSample data:\n       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  \nPrediction results without fine-tuning:\n\texact_match: 0.37%\n\tf1_score: 1.70%\n\tavg_jaccard: 0.01\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"processor = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:54:20.367934Z","iopub.execute_input":"2025-01-29T13:54:20.368298Z","iopub.status.idle":"2025-01-29T13:55:31.262659Z","shell.execute_reply.started":"2025-01-29T13:54:20.368271Z","shell.execute_reply":"2025-01-29T13:55:31.261939Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Initializing data processor...\n\nPreparing datasets...\nDataset columns: ['textID', 'text', 'selected_text', 'sentiment']\n\nSample data:\n       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  \nTrain dataset size: 24732\nEval dataset size: 2748\n\nSetting up model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50c1e54aae64dd0b23873c44010c9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45018d33e70442c3bd3a1d8e22c4d679"}},"metadata":{}},{"name":"stdout","text":"\nSelecting Dataset({\n    features: ['textID', 'text', 'selected_text', 'sentiment', '__index_level_0__', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n    num_rows: 24732\n}) training samples and Dataset({\n    features: ['textID', 'text', 'selected_text', 'sentiment', '__index_level_0__', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n    num_rows: 2748\n}) evaluation samples...\n\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"`evaluation_strategy` is deprecated and will be removed in version 4.46 of ð¤ Transformers. Use `eval_strategy` instead\n`tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [96/96 00:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Match</th>\n      <th>F1 Score</th>\n      <th>Avg Jaccard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.805752</td>\n      <td>9.00%</td>\n      <td>44.81%</td>\n      <td>0.37</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.130236</td>\n      <td>29.00%</td>\n      <td>59.22%</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.948870</td>\n      <td>34.00%</td>\n      <td>60.46%</td>\n      <td>0.54</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nTraining completed successfully!\n\nSaving model...\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"prediction_test(processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:55:32.682279Z","iopub.execute_input":"2025-01-29T13:55:32.682621Z","iopub.status.idle":"2025-01-29T13:55:32.696485Z","shell.execute_reply.started":"2025-01-29T13:55:32.682590Z","shell.execute_reply":"2025-01-29T13:55:32.695525Z"}},"outputs":[{"name":"stdout","text":"Predicted sentiment span: you guys didn ` t say hi or answer my questions yesterday but nice songs.\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"def trainFromCheckpoint(checkpoint):\n    # Initialize processor\n    print(\"Initializing data processor...\")\n    processor = SentimentExtraction('/kaggle/input/tweet-extraction/train.csv')\n\n    processor.train(checkpoint)\n\n\ntrainFromCheckpoint('/kaggle/working/finetune-BERT-tweet/checkpoint-8000')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"font-size: 22px;\">From the Hugging Face Transformers library, acquires a pre-trained BERT-based sentiment analysis model (**LLM**), with its appropriate pipeline (**classifier**) and **tokenizer**.</span>\n","metadata":{"id":"vKgnOStR-t9a"}},{"cell_type":"markdown","source":"### Predictions sentiment extraction","metadata":{}},{"cell_type":"markdown","source":"<span style=\" color: yellow; font-size: 24px;\">Importan Note by Sam: </span><span style=\"font-size: 22px;\">LLMs such as BERT, LLama, GPTT,etc., and BERT-based models (like distilbert-base-uncased-finetuned-sst-2-english) do not include an activation function (e.g., softmax or sigmoid) at their output layer by default. Instead, they output logits, which are raw, unnormalized scores for each class. The activation function, if needed, is applied externally depending on the specific task and requirements.<br> BERT is a general-purpose transformer model. By omitting the activation function, it can be fine-tuned for various tasks: Multi-class classification (e.g., softmax), Binary classification (e.g., sigmoid), Regression tasks (no activation function).<br></span>\n\n\n<span style=\" color: yellow; font-size: 24px;\">outputs</span> <span style=\"font-size: 22px;\">It is an object of type ModelOutput, specifically tailored for the model we are using. Here we didn't simply print the result of the sentiment for the each tweet, but we used output.logits to go access the detailed classification results for each text for further word by word analysis in SHAP.<br></span>\n<span style=\" color: yellow; font-size: 24px;\">logits:</span> <span style=\"font-size: 22px;\"> is the modelâs raw output before applying the activation function. logits are unnormalized scores, which can be positive, negative, or zero, depending on the modelâs confidence in each class. Here \"outputs.logits\" is a tensor containing the raw, unnormalized predictions for each input sequence. Each row in the tensor corresponds to one input in texts, and each column corresponds to a class.<br></span>\n<span style=\" color: yellow; font-size: 24px;\">Normalize logits to probabilities:</span><span style=\"font-size: 22px;\"> by applying softmax we normalize the raw predictions for each tweet to probabilities (the results are printed for clarity).<br></span>\n<span style=\" color: yellow; font-size: 24px;\">detach()</span><span style=\"font-size: 22px;\"> We use it since we donât want to compute gradients for the tensor. Since probabilities are used only for prediction (not training), thereâs no need to track gradients, so detach() avoids unnecessary computational overhead.<br></span>\n","metadata":{"id":"Ubw4qATO-t9a"}},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n# Load pre-trained sentiment analysis model and tokenizer\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nclassifier = pipeline(\"sentiment-analysis\", model=model_name)\n\n# Tokenizer and model for SHAP\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a wrapper for the classifier to work with SHAP\ndef classifier_wrapper(texts):\n    tokens = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n    outputs = model(**tokens)\n    probabilities = outputs.logits.softmax(dim=1).detach().numpy()\n    print(f\"\\nlogits contains tweets' unnormalized prediction scores for each class(positive / negative):\\n{outputs.logits}\\n\")\n    return probabilities\n\n# Select a subset of the training data for SHAP analysis\nsample_texts = train_df[\"text\"].sample(10, random_state=42).tolist()\nprint(\"Sample Texts:\")\nprint(sample_texts)\nprint(f\"Probibilities after normalizing logits by softwax:\\n {classifier_wrapper(sample_texts)}\")  # Should output a numpy array of probabilities\n","metadata":{"id":"13vHVSQW-t9a","outputId":"01282d8b-eeee-420c-926b-1302f1db0109","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\" color: yellow; font-size: 24px;\">SHAP Analysis:</span><span style=\"font-size: 22px;\"> SHAP is based on <u>Shapley values</u> which is a concept from <u>Game Theory</u>. Imagine a group of players collaborating to achieve a common goal. Shapley values determine how to fairly distribute the rewards or profits among the players based on their individual contributions.\nIn the context of machine learning, features of a data point can be seen as \"players\" contributing to the model's prediction.<br> SHAP aims to fairly attribute the \"credit\" (or \"blame\") for the model's output to each feature.<br>\nFor complex models, SHAP often uses <u>approximations</u> or <u>sampling techniques</u> to estimate Shapley values.<br>\nThe explainer considers <u>all possible combinations of features (words or tokens)</u> in the sample.\nFor each combination, it calculates the model's prediction.<br>\nThis process helps determine how much each feature contributes to the overall prediction.\nShapley Values are assigned based on the calculated predictions for different feature combinations,\nSHAP assigns a value to each feature.\n</br></br></spn>\n</span>\n<span style=\" color: yellow; font-size: 24px;\">shap.Explainer:</span><span style=\"font-size: 22px;\"> Is the core class in the SHAP library. It encapsulates the model and the method for <u>calculating</u> SHAP values, which quantify the <u>contribution of each feature</u> (in this case, words or tokens) <u>to the model's prediction</u> for a given sample.<br>\nThe Explainer needs \"classifier_wrapper\" function to understand how the model makes predictions.<br><br></span>\n\n<span style=\" color: yellow; font-size: 24px;\">Note: </span><span style=\"font-size: 22px;\">\nWhen classifier_wrapper is run by SHAP, there are six or less probibilities for the ten tweets in sample_texts. It is because the tokenizer applies padding and truncation to the input. If multiple texts are identical, the tokenizer may return the same tokens for duplicates. If sample_texts contains duplicate or very similar tweets, their logits will be identical, and we might get fewer output tensors.<br><br></span>","metadata":{"id":"hF9hc2bz-t9a"}},{"cell_type":"code","source":"\n# Explain predictions using SHAP\n# This line integrates the model in the function to SHAP and encapsulates\n# the necessary information to compute SHAP values for the given model and data.\nexplainer = shap.Explainer(classifier_wrapper, tokenizer)\nshap_values = explainer(sample_texts)\n\n# Visualize the SHAP values for each word:\n# Output 0 and Output 1: predicted probabilities for two classes\nfor i, text in enumerate(sample_texts):\n    print(f\"\\nSHAP Analysis for Text {i + 1}: {text}\")\n    shap.plots.text(shap_values[i])\n'''\n#The following code save SHAP analysis results into different html files for each tweet:\n\n#shap.save_html(\"shap_analysis.html\", shap_values)\n# Manually set the base value if explainer.expected_value is None\n# For classification tasks, the base value can be the mean of logits\nif explainer.expected_value is None:\n    print(\"explainer.expected_value is None. Computing a manual base value.\")\n    base_value = np.mean([np.mean(values.values) for values in shap_values])\nelse:\n    base_value = explainer.expected_value\n\n# Loop through all SHAP values and save each as a separate force plot\nfor i, shap_value in enumerate(shap_values):\n    # Debugging: Print base value and SHAP values\n    print(f\"Processing Text {i + 1}\")\n    print(f\"Base Value: {base_value}\")\n    print(f\"SHAP Values for Text {i + 1}: {shap_value.values}\")\n\n    # Check if shap_value.values is None\n    if shap_value.values is None:\n        print(f\"Skipping Text {i + 1} due to missing values.\")\n        continue\n\n    # Handle multi-output models (if applicable)\n    if isinstance(base_value, list):\n        base_value = base_value[0]  # Use the base value for the first output\n\n    # Create a force plot for the current sample\n    try:\n        vis = shap.plots.force(base_value, shap_value.values)\n\n        # Save the visualization as an HTML file\n        file_name = f\"shap_analysis_{i + 1}.html\"\n        shap.save_html(file_name, vis)\n        print(f\"Saved SHAP visualization for Text {i + 1} as {file_name}\")\n    except Exception as e:\n        print(f\"Error processing Text {i + 1}: {e}\")\n'''","metadata":{"colab":{"referenced_widgets":["2bdfb5713df34018959bf3f4e89e548b","9d94eb89faaa421dadcaf0e2181df74d","305de2c39a00463b9552380aec2c78ef","a104b74d08d540398965e35fd021b540","13f142a64ce54aa1ba4be86e83fed3f2","6864e9a6e7774f848f563b8fdda6f4e5","ed52ddcdc74049e99fb2cdaa1fbf9961"]},"id":"UZ0Zfs2--t9a","outputId":"618aaa65-2e38-4285-9ed4-da05a7e0112b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r checkpoint-9276.zip /kaggle/working/finetune-BERT-tweet/checkpoint-9276","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}